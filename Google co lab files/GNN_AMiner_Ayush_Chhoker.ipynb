{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GNN Experiments on AMiner Dataset\n",
        "\n",
        "Testing out different GNN architectures (GAT, GCN, GraphSAGE) on the AMiner author collaboration network. \n",
        "This dataset has around 10k authors across 8 research fields.\n",
        "\n",
        "**Goal:** See which architecture works best for classifying authors by their research area based on collaboration patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setting up the environment - this takes a minute\n",
        "!pip install -q torch torchvision\n",
        "!pip install -q torch-geometric\n",
        "!pip install -q ogb\n",
        "!pip install -q matplotlib seaborn pandas numpy scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GATConv, GCNConv, SAGEConv\n",
        "from torch_geometric.data import Data\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading AMiner Data\n",
        "\n",
        "The AMiner dataset contains author collaboration networks. Each node is an author, edges represent collaborations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch_geometric.datasets import AMiner\n",
        "import torch_geometric.transforms as T\n",
        "\n",
        "# Load the dataset\n",
        "print(\"Loading AMiner dataset...\")\n",
        "dataset = AMiner(root='/tmp/AMiner', transform=T.NormalizeFeatures())\n",
        "data = dataset[0]\n",
        "\n",
        "# Basic stats\n",
        "print(f\"\\nDataset stats:\")\n",
        "print(f\"  Nodes (authors): {data.num_nodes}\")\n",
        "print(f\"  Edges: {data.num_edges}\")\n",
        "print(f\"  Features per node: {data.num_features}\")\n",
        "print(f\"  Research fields: {dataset.num_classes}\")\n",
        "print(f\"  Average degree: {data.num_edges / data.num_nodes:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick preprocessing - adding self-loops helps with message passing\n",
        "from torch_geometric.utils import add_self_loops, remove_self_loops\n",
        "\n",
        "edge_index = data.edge_index\n",
        "edge_index, _ = remove_self_loops(edge_index)\n",
        "edge_index, _ = add_self_loops(edge_index, num_nodes=data.num_nodes)\n",
        "data.edge_index = edge_index\n",
        "\n",
        "print(f\"After adding self-loops: {data.num_edges} edges\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating Train/Val/Test Splits\n",
        "\n",
        "Using 60-20-20 split. Trying to keep it balanced across classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create masks for train/val/test\n",
        "num_authors = data.num_nodes\n",
        "num_classes = dataset.num_classes\n",
        "\n",
        "train_mask = torch.zeros(num_authors, dtype=torch.bool)\n",
        "val_mask = torch.zeros(num_authors, dtype=torch.bool)\n",
        "test_mask = torch.zeros(num_authors, dtype=torch.bool)\n",
        "\n",
        "# Split per class to keep it balanced\n",
        "for c in range(num_classes):\n",
        "    class_indices = (data.y == c).nonzero(as_tuple=True)[0]\n",
        "    n = len(class_indices)\n",
        "    \n",
        "    perm = torch.randperm(n)\n",
        "    train_size = int(0.6 * n)\n",
        "    val_size = int(0.2 * n)\n",
        "    \n",
        "    train_mask[class_indices[perm[:train_size]]] = True\n",
        "    val_mask[class_indices[perm[train_size:train_size+val_size]]] = True\n",
        "    test_mask[class_indices[perm[train_size+val_size:]]] = True\n",
        "\n",
        "data.train_mask = train_mask\n",
        "data.val_mask = val_mask\n",
        "data.test_mask = test_mask\n",
        "\n",
        "print(f\"Train: {train_mask.sum()} | Val: {val_mask.sum()} | Test: {test_mask.sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Architectures\n",
        "\n",
        "Implementing three different GNN models to compare:\n",
        "1. **GAT** - uses attention mechanisms\n",
        "2. **GCN** - classic graph convolution\n",
        "3. **GraphSAGE** - neighborhood sampling approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GAT(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, heads=4, dropout=0.6):\n",
        "        super().__init__()\n",
        "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout)\n",
        "        self.conv2 = GATConv(hidden_channels * heads, out_channels, heads=1, concat=False, dropout=dropout)\n",
        "        self.dropout = dropout\n",
        "    \n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.elu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.6):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
        "        self.dropout = dropout\n",
        "    \n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "class GraphSAGE(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.6):\n",
        "        super().__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
        "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
        "        self.dropout = dropout\n",
        "    \n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "print(\"Models defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Setup\n",
        "\n",
        "Using class weights to handle imbalanced data - some research fields have way more authors than others."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, data, model_name, epochs=200, lr=0.01, weight_decay=5e-4, patience=40):\n",
        "    \"\"\"\n",
        "    Training loop with early stopping\n",
        "    \"\"\"\n",
        "    model = model.to(device)\n",
        "    data = data.to(device)\n",
        "    \n",
        "    # Calculate class weights for imbalanced data\n",
        "    class_counts = torch.bincount(data.y[data.train_mask])\n",
        "    class_weights = 1.0 / class_counts.float()\n",
        "    class_weights = class_weights / class_weights.sum() * len(class_weights)\n",
        "    class_weights = class_weights.to(device)\n",
        "    \n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    criterion = nn.NLLLoss(weight=class_weights)\n",
        "    \n",
        "    best_val_acc = 0\n",
        "    best_model_state = None\n",
        "    patience_counter = 0\n",
        "    \n",
        "    train_losses = []\n",
        "    val_accs = []\n",
        "    \n",
        "    print(f\"\\nTraining {model_name}...\")\n",
        "    \n",
        "    for epoch in range(1, epochs + 1):\n",
        "        # Training\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index)\n",
        "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            out = model(data.x, data.edge_index)\n",
        "            pred = out.argmax(dim=1)\n",
        "            val_acc = (pred[data.val_mask] == data.y[data.val_mask]).sum().item() / data.val_mask.sum().item()\n",
        "        \n",
        "        train_losses.append(loss.item())\n",
        "        val_accs.append(val_acc)\n",
        "        \n",
        "        # Early stopping\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_model_state = model.state_dict().copy()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "        \n",
        "        if epoch % 20 == 0:\n",
        "            print(f\"Epoch {epoch:03d} | Loss: {loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "        \n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch}\")\n",
        "            break\n",
        "    \n",
        "    # Load best model\n",
        "    model.load_state_dict(best_model_state)\n",
        "    \n",
        "    return model, train_losses, val_accs\n",
        "\n",
        "def evaluate_model(model, data):\n",
        "    \"\"\"\n",
        "    Get test set metrics\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(data.x, data.edge_index)\n",
        "        pred = out.argmax(dim=1)\n",
        "        \n",
        "        test_acc = (pred[data.test_mask] == data.y[data.test_mask]).sum().item() / data.test_mask.sum().item()\n",
        "        \n",
        "        # Get predictions for metrics\n",
        "        y_true = data.y[data.test_mask].cpu().numpy()\n",
        "        y_pred = pred[data.test_mask].cpu().numpy()\n",
        "        \n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    \n",
        "    return test_acc, precision, recall, f1, y_true, y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training GAT Model\n",
        "\n",
        "Starting with GAT - attention mechanism should help with varying node degrees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gat = GAT(data.num_features, 256, num_classes, heads=4, dropout=0.6)\n",
        "gat_model, gat_losses, gat_val_accs = train_model(gat, data, 'GAT', epochs=200, lr=0.005)\n",
        "gat_test_acc, gat_precision, gat_recall, gat_f1, gat_y_true, gat_y_pred = evaluate_model(gat_model, data)\n",
        "\n",
        "print(f\"\\nGAT Results:\")\n",
        "print(f\"  Test Accuracy: {gat_test_acc:.4f}\")\n",
        "print(f\"  Precision: {gat_precision:.4f}\")\n",
        "print(f\"  Recall: {gat_recall:.4f}\")\n",
        "print(f\"  F1 Score: {gat_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training GCN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gcn = GCN(data.num_features, 256, num_classes, dropout=0.6)\n",
        "gcn_model, gcn_losses, gcn_val_accs = train_model(gcn, data, 'GCN', epochs=200, lr=0.01)\n",
        "gcn_test_acc, gcn_precision, gcn_recall, gcn_f1, gcn_y_true, gcn_y_pred = evaluate_model(gcn_model, data)\n",
        "\n",
        "print(f\"\\nGCN Results:\")\n",
        "print(f\"  Test Accuracy: {gcn_test_acc:.4f}\")\n",
        "print(f\"  Precision: {gcn_precision:.4f}\")\n",
        "print(f\"  Recall: {gcn_recall:.4f}\")\n",
        "print(f\"  F1 Score: {gcn_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training GraphSAGE Model\n",
        "\n",
        "GraphSAGE uses neighborhood sampling - curious to see how it compares"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sage = GraphSAGE(data.num_features, 256, num_classes, dropout=0.6)\n",
        "sage_model, sage_losses, sage_val_accs = train_model(sage, data, 'GraphSAGE', epochs=200, lr=0.01)\n",
        "sage_test_acc, sage_precision, sage_recall, sage_f1, sage_y_true, sage_y_pred = evaluate_model(sage_model, data)\n",
        "\n",
        "print(f\"\\nGraphSAGE Results:\")\n",
        "print(f\"  Test Accuracy: {sage_test_acc:.4f}\")\n",
        "print(f\"  Precision: {sage_precision:.4f}\")\n",
        "print(f\"  Recall: {sage_recall:.4f}\")\n",
        "print(f\"  F1 Score: {sage_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparing Results\n",
        "\n",
        "Let's see which model performed best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary table\n",
        "results_df = pd.DataFrame({\n",
        "    'Model': ['GAT', 'GCN', 'GraphSAGE'],\n",
        "    'Test Accuracy': [gat_test_acc, gcn_test_acc, sage_test_acc],\n",
        "    'Precision': [gat_precision, gcn_precision, sage_precision],\n",
        "    'Recall': [gat_recall, gcn_recall, sage_recall],\n",
        "    'F1 Score': [gat_f1, gcn_f1, sage_f1]\n",
        "})\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(results_df.to_string(index=False))\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizations\n",
        "\n",
        "Plotting training curves and confusion matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training curves\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Loss curves\n",
        "axes[0].plot(gat_losses, label='GAT', alpha=0.7)\n",
        "axes[0].plot(gcn_losses, label='GCN', alpha=0.7)\n",
        "axes[0].plot(sage_losses, label='GraphSAGE', alpha=0.7)\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Training Loss')\n",
        "axes[0].set_title('Training Loss Over Time')\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# Validation accuracy\n",
        "axes[1].plot(gat_val_accs, label='GAT', alpha=0.7)\n",
        "axes[1].plot(gcn_val_accs, label='GCN', alpha=0.7)\n",
        "axes[1].plot(sage_val_accs, label='GraphSAGE', alpha=0.7)\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Validation Accuracy')\n",
        "axes[1].set_title('Validation Accuracy Over Time')\n",
        "axes[1].legend()\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion matrices for best model\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "models_data = [\n",
        "    ('GAT', gat_y_true, gat_y_pred),\n",
        "    ('GCN', gcn_y_true, gcn_y_pred),\n",
        "    ('GraphSAGE', sage_y_true, sage_y_pred)\n",
        "]\n",
        "\n",
        "for idx, (name, y_true, y_pred) in enumerate(models_data):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx])\n",
        "    axes[idx].set_title(f'{name} Confusion Matrix')\n",
        "    axes[idx].set_xlabel('Predicted')\n",
        "    axes[idx].set_ylabel('Actual')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Saving Models\n",
        "\n",
        "Saving the trained models for later use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save models\n",
        "torch.save({\n",
        "    'gat_state_dict': gat_model.state_dict(),\n",
        "    'gcn_state_dict': gcn_model.state_dict(),\n",
        "    'sage_state_dict': sage_model.state_dict(),\n",
        "    'num_features': data.num_features,\n",
        "    'num_classes': num_classes,\n",
        "    'results': results_df.to_dict()\n",
        "}, 'aminer_models.pt')\n",
        "\n",
        "print(\"Models saved to aminer_models.pt\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}